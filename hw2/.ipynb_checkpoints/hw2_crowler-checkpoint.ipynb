{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Краулер, который собирает тексты с новостного ресурса.\n",
    "Выбираете себе одну газету и скачиваете оттуда статьи для корпуса. Вид статьи описан в доке. Давайте статей будет до 1000. (http://www.belrab.ru/)\n",
    "\n",
    "\n",
    "!!!Не стала выводить автора, т.к в разных статьях он в разных местах по тексту, хотя можно было бы использовать регистр, т.к. фамилия автора всегда написана большими буквами. Но точно так же маркируется фамилия фотографа и т.п. и в той же части, где автор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.belrab.ru/rubriki/politika/item/2135-deputat-pomogaet-detyam\n"
     ]
    }
   ],
   "source": [
    "def article_links(archieve_url):\n",
    "    n = 0\n",
    "\n",
    "    article_links = []\n",
    "\n",
    "    archieve_page_url = 'http://www.belrab.ru' + archieve_url + '?start=' + str(n)\n",
    "    req = requests.get(archieve_page_url)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    end_page = 'http://www.belrab.ru' + soup.find('a', attrs = {'title':'В конец'})['href']\n",
    "    \n",
    "    \n",
    "    while archieve_page_url != end_page:\n",
    "        \n",
    "        archieve_page_url = 'http://www.belrab.ru' + archieve_url + '?start=' + str(n)\n",
    "        req = requests.get(archieve_page_url)\n",
    "        soup = BeautifulSoup(req.text, 'lxml')\n",
    "        \n",
    "        for h in soup.findAll('div', attrs = {'class':'catItemImageBlock'}):\n",
    "            article_l = h.find('a')\n",
    "            if article_l:\n",
    "                article_l = article_l['href']\n",
    "                article_links.append('http://www.belrab.ru' + article_l)\n",
    "\n",
    "            \n",
    "        n += 8\n",
    "    return article_links  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_info_article(article_link):\n",
    "    '''function creating a file with text from an article page'''\n",
    "    req = requests.get(article_link)\n",
    "    soup = BeautifulSoup(req.text,'lxml')   \n",
    "    title = soup.find('meta', attrs = {'name':'title'})['content']\n",
    "    date = soup.find('div',  attrs = {'class':'date-vote-thor'}).text.strip()\n",
    "    \n",
    "    for t in soup.findAll('a', attrs = {'class':'active'}):\n",
    "        if 'Рубрики' not in t['title']:\n",
    "            topic = t['title']\n",
    "    \n",
    "    art_text = '\\n\\r'.join((soup.find('div',  attrs = {'class':'itemIntroText'}).text.strip(), \n",
    "                            soup.find('div',  attrs = {'class':'itemFullText'}).text.strip()))\n",
    "    \n",
    "    ready_text =  '\\n\\r'.join(('@au Noname #Too hard to extract from text', ('@ti\\t' + title),\n",
    "                              ('@da\\t' + date), ('@topic\\t' + topic),('@url\\t' + article_link), ('\\n\\r' + art_text)))\n",
    "    \n",
    "    return ready_text\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.belrab.ru/rubriki/politika?limitstart=0\n",
    "def belrab(number_articles_wanted):\n",
    "    '''function finding links for category pages on the site, downloading all article links from  those, downloading as much \n",
    "    articles as you state in number_articles_wanted argument'''\n",
    "    num_art = 1\n",
    "        \n",
    "    url = 'http://www.belrab.ru/'\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    \n",
    "    for category in soup.findAll('li', attrs = {'class':'level2'}):\n",
    "        category_link = category.find('a')\n",
    "        if category_link:\n",
    "            for article_link in article_links(category_link['href']):\n",
    "                while num_art != (number_articles_wanted + 1):\n",
    "                    with open(str(num_art) + '.txt','w',encoding='utf-8') as f:\n",
    "                        f.write(find_info_article(article_link))\n",
    "                        num_art += 1\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "belrab(1000)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
